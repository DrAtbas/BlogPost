# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.16.1
#   kernelspec:
#     display_name: .venv
#     language: python
#     name: python3
# ---

# # Detecting Text Generated by AI/Human
#
# An example of how to detect text that is generated by AI/Human.

# ## Importing Necessary Libraries
#
# In this code snippet, essential Python libraries are imported to support various tasks related to natural language processing (NLP), machine learning, and evaluation metrics. Here's an explanation of the provided code:
#
# - `pandas as pd`: The `pandas` library is imported as 'pd' for data manipulation and analysis. It provides data structures and functions for efficiently handling structured data, such as tabular data.
#
# - `tokenizers`: The `tokenizers` library is imported to facilitate text tokenization, which is a crucial preprocessing step in NLP tasks. It offers various tokenization algorithms, pre-tokenizers, and models for creating custom tokenization pipelines.
#
# - `datasets`: The `datasets` library is imported to work with datasets commonly used in NLP tasks. It provides convenient interfaces for loading and preprocessing datasets, making it easier to train and evaluate machine learning models.
#
# - `transformers`: The `transformers` library is imported to leverage pre-trained transformer-based models for NLP tasks. It offers a wide range of pre-trained models and tools for fine-tuning them on custom datasets.
#
# - `tqdm`: The `tqdm` library is imported to display progress bars for iterative processes, enhancing the user experience by providing real-time feedback on the progress of tasks such as data processing and model training.
#
# - `sklearn`: The `sklearn` (scikit-learn) library is a comprehensive machine learning library that provides tools for various tasks, including feature extraction, model selection, and evaluation metrics.
#
#     - `TfidfVectorizer`: This class from the `sklearn.feature_extraction.text` module is used to convert a collection of raw documents into a matrix of TF-IDF (Term Frequency-Inverse Document Frequency) features. TF-IDF is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents.
#
#     - `MultinomialNB`: This class from the `sklearn.naive_bayes` module implements the Multinomial Naive Bayes classifier, which is commonly used for text classification tasks such as spam detection.
#
#     - `SGDClassifier`: This class from the `sklearn.linear_model` module implements the Stochastic Gradient Descent (SGD) classifier, which is a linear classifier optimized using SGD. It's suitable for large-scale machine learning tasks and can efficiently handle sparse data.
#
#     - `VotingClassifier`: This class from the `sklearn.ensemble` module allows combining multiple classifiers and using a majority vote or weighted voting scheme to make predictions. It can improve prediction accuracy by leveraging diverse models.
#
# - `numpy as np`: The `numpy` library is imported as 'np' for numerical computing. It provides support for multidimensional arrays, mathematical functions, linear algebra operations, and random number generation, making it essential for many scientific computing tasks.
#
# - `classification_report`, `confusion_matrix`, `accuracy_score`: These functions from the `sklearn.metrics` module are used for evaluating the performance of classification models. They provide metrics such as precision, recall, F1-score, confusion matrix, and accuracy, which help assess the model's effectiveness on test data.
#

# +
import pandas as pd
from tokenizers import Tokenizer, models, normalizers, pre_tokenizers
from tokenizers import trainers
from datasets import Dataset
from transformers import PreTrainedTokenizerFast
from tqdm import tqdm
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB # For Spam Detection, Could be useful?
from sklearn.linear_model import SGDClassifier # Stochastic Gradient Descent, find a linear fit?
from sklearn.ensemble import VotingClassifier # To combine the models voting scheme
import numpy as np

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
# -

# ## Loading Data
#
# In this section of the code, data is loaded from CSV files into Pandas DataFrames for further processing. Here's an explanation of the provided code:
#
# - `pd.read_csv('Data/LLM/test_essays.csv', sep=',')`: This line of code reads the data from the CSV file named 'test_essays.csv' located in the 'Data/LLM' directory into a Pandas DataFrame named `org_test`. The `sep=','` parameter specifies that the data is comma-separated.
#
# - `pd.read_csv('Data/LLM/sample_submission.csv', sep=',')`: This line of code reads the data from the CSV file named 'sample_submission.csv' located in the 'Data/LLM' directory into a Pandas DataFrame named `sub`. Similarly, the `sep=','` parameter specifies that the data is comma-separated.
#
# - `pd.read_csv('Data/LLM/train_essays.csv', sep=',')`: This line of code reads the data from the CSV file named 'train_essays.csv' located in the 'Data/LLM' directory into a Pandas DataFrame named `org_train`. Once again, the `sep=','` parameter specifies that the data is comma-separated.
#
# These lines of code load the necessary data for training and testing machine learning models from CSV files into Pandas DataFrames, enabling further exploration, preprocessing, and modeling tasks.

org_test = pd.read_csv('Data/LLM/test_essays.csv', sep=',')
sub = pd.read_csv('Data/LLM/sample_submission.csv', sep=',')
org_train = pd.read_csv('Data/LLM/train_essays.csv', sep=',')

# ## Additional Training Data
#
# In this section of the code, additional training data is loaded from multiple CSV files into separate Pandas DataFrames for potential integration with the existing training data. Here's an explanation of the provided code:
#
# - `pd.read_csv("Data/Additional/train_drcat_01.csv", sep=',')`: This line of code reads the data from the CSV file named 'train_drcat_01.csv' located in the 'Data/Additional' directory into a Pandas DataFrame named `train1`. The `sep=','` parameter specifies that the data is comma-separated.
#
# - `pd.read_csv("Data/Additional/train_drcat_02.csv", sep=',')`: Similarly, this line of code reads the data from the CSV file named 'train_drcat_02.csv' located in the 'Data/Additional' directory into a Pandas DataFrame named `train2`.
#
# - `#train3 = pd.read_csv("Data/Additional/train_drcat_03.csv", sep=',')`: There are two additional lines of code (commented out) for loading more training data from files 'train_drcat_03.csv' and 'train_drcat_04.csv'. These lines can be uncommented and modified as needed if more data files are available.
#
# These lines of code load additional training data from separate CSV files, potentially increasing the size and diversity of the training dataset, which can improve the performance of machine learning models trained on this data.

# +
train1 = pd.read_csv("Data/Additional/train_drcat_01.csv", sep=',')

train2 = pd.read_csv("Data/Additional/train_drcat_02.csv", sep=',')

#train3 = pd.read_csv("Data/Additional/train_drcat_03.csv", sep=',')

#train4 = pd.read_csv("Data/Additional/train_drcat_04.csv", sep=',')
# -

# ## Data Concatenation and Cleaning
#
# In this section of the code, the loaded training data is concatenated and cleaned. Here's an explanation of the provided code:
#
# - `#train = pd.concat([train1, train2, train3, train4], ignore_index=True, sort=False)`: This line of code (commented out) concatenates multiple DataFrames `train1`, `train2`, `train3`, and `train4` into a single DataFrame `train`. The `pd.concat()` function is used with the `ignore_index=True` parameter to reset the index of the concatenated DataFrame and `sort=False` to avoid sorting the columns. This operation combines data from multiple sources into one DataFrame, potentially increasing the size and diversity of the training dataset.
#
# - `train = train2`: This line of code assigns the DataFrame `train2` to the variable `train`, effectively selecting only one of the loaded datasets for further processing. This may be done if the user decides to use only a specific subset of the available data for training.
#
# - `test = train1`: Similarly, this line of code assigns the DataFrame `train1` to the variable `test`, indicating that `train1` will be used as a test dataset.
#
# - `train = train.drop_duplicates(subset="text", keep='first')`: This line of code removes duplicate rows from the `train` DataFrame based on the values in the 'text' column. The `drop_duplicates()` function is used with the `subset="text"` parameter to specify that duplicates should be identified based on the 'text' column. The `keep='first'` parameter ensures that only the first occurrence of each duplicated row is kept, while subsequent occurrences are removed.
#
# These operations facilitate data cleaning and preparation by ensuring that the training data is free from duplicate entries, potentially improving the quality and performance of machine learning models trained on this data.

# +
#train = pd.concat([train1, train2, train3, train4], ignore_index=True, sort=False)

train = train2
test = train1


train = train.drop_duplicates(subset="text", keep='first')
# -

# ## Data Preparation for Testing
#
# In this section of the code, the testing data is prepared for evaluation. Here's an explanation of the provided code:
#
# - `test.reset_index(drop=True, inplace=True)`: This line of code resets the index of the `test` DataFrame after potentially removing rows or reordering them. The `drop=True` parameter specifies that the old index should be discarded, and `inplace=True` ensures that the changes are made directly to the DataFrame without creating a new copy.
#
# - `test.insert(loc=0, column='prompt_id', value=range(2, len(test) + 2))`: This line of code inserts a new column named 'prompt_id' at the beginning of the `test` DataFrame. The values for this column are generated using the `range()` function, starting from 2 and ending at the length of the DataFrame plus 2. This step potentially assigns unique prompt IDs to each row in the test dataset.
#
# - `test.insert(loc=0, column='id', value=test.index)`: Similarly, this line of code inserts a new column named 'id' at the beginning of the `test` DataFrame. The values for this column are assigned based on the index of each row in the DataFrame, potentially creating a unique identifier for each row.
#
# - `labels = test['label']`: This line of code extracts the labels (if available) from the 'label' column of the `test` DataFrame and assigns them to a variable named `labels`. This step is essential for evaluating the performance of machine learning models on the test dataset.
#
# - `test = test.drop("label", axis=1)`: This line of code removes the 'label' column from the `test` DataFrame, as it is typically used for training but not for testing. The `drop()` function is used with `axis=1` to indicate that the column should be dropped along the vertical axis (columns).
#
# - `print(test)`: Finally, this line of code prints the modified `test` DataFrame, showing the result of the data preparation steps.
#
# These operations prepare the testing data by resetting the index, adding unique identifiers for each row, extracting labels (if available), and removing unnecessary columns, ensuring that the data is ready for evaluation with machine learning models.
#
# This is done to make the test dataset look same as train dataset.

test.reset_index(drop=True, inplace=True)
test.insert(loc=0, column='prompt_id', value=range(2, len(test) + 2))  
test.insert(loc=0, column='id', value=test.index) 
labels = test['label']
test = test.drop("label", axis=1)
print(test)

# ## Data Preparation for Training
#
# In this section of the code, the training data is prepared for model training. Here's an explanation of the provided code:
#
# - `train.reset_index(drop=True, inplace=True)`: This line of code resets the index of the `train` DataFrame after potentially removing rows or reordering them. The `drop=True` parameter specifies that the old index should be discarded, and `inplace=True` ensures that the changes are made directly to the DataFrame without creating a new copy.
#
# This operation resets the index of the training data, ensuring that it is continuous and starts from zero, which is a common practice before training machine learning models.

train.reset_index(drop=True, inplace=True)

# ## Vocabulary Size and Lowercase Setting
#
# In this section of the code, parameters related to text processing are defined. Here's an explanation of the provided code:
#
# - `vocabulary_size = 46850`: This line of code sets the vocabulary size to 46850. This value is derived from a source cited in the comment, which suggests taking the average of the high and median vocabulary sizes reported in the source. The resulting value, 46850, is used as the vocabulary size for text processing tasks.
#
# - `LOWERCASE = False`: This line of code sets the lowercase parameter to `False`. This parameter determines whether the text should be converted to lowercase during text processing tasks such as tokenization or feature extraction. By setting it to `False`, the original casing of the text will be preserved.
#
# These parameters play a crucial role in text processing and feature extraction tasks, influencing the size of the vocabulary and the treatment of text casing during preprocessing.

vocabulary_size = 46850  # Derived from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4965448/ add 20 year old High (End(51,700) + Median(42,000))/2, 46850 is the result.
LOWERCASE = False # I think this needs to be here rather than above.

# ## Tokenizer Configuration
#
# In this section of the code, the tokenizer is configured for preprocessing text data. Here's an explanation of the provided code:
#
# - `special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]`: This line of code defines a list of special tokens to be used by the tokenizer. These tokens serve specific purposes during tokenization and are often used in natural language processing tasks. The special tokens defined here include tokens for representing unknown words, padding, classification, separation, and masking.
#
# - `raw_tokenizer = Tokenizer(models.BPE(unk_token= "[UNK]"))`: This line of code initializes the tokenizer with a Byte Pair Encoding (BPE) model. BPE is a tokenization algorithm commonly used in natural language processing tasks. The `unk_token="[UNK]"` parameter specifies the token to be used for representing unknown words encountered during tokenization.
#
# - `raw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])`: This line of code configures the normalizer for the tokenizer. The normalizer is responsible for applying preprocessing steps to the input text before tokenization. In this case, a sequence of normalizers is defined, including NFC normalization (Unicode normalization form C) and lowercase conversion if the `LOWERCASE` parameter is set to `True`. This step ensures consistent and standardized text input for tokenization.
#
# - `raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()`: Finally, this line of code sets the pre-tokenizer for the tokenizer. The pre-tokenizer defines how the input text should be split into initial tokens before further processing. In this case, the ByteLevel pre-tokenizer is used, which splits the text into byte-level tokens. This pre-tokenization step is commonly used in conjunction with tokenization algorithms like BPE.
#
# These configuration settings prepare the tokenizer for preprocessing text data, ensuring that it can effectively tokenize and process input text according to the defined specifications.

special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
raw_tokenizer = Tokenizer(models.BPE(unk_token= "[UNK]")) # Byte Pair Encoding
raw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])  # Normalise the text
raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()

# ## BPE Trainer Configuration
#
# In this section of the code, the trainer for Byte Pair Encoding (BPE) is configured. Here's an explanation of the provided code:
#
# - `trainer = trainers.BpeTrainer(vocab_size=vocabulary_size, special_tokens=special_tokens)`: This line of code initializes the BPE trainer with specific configuration parameters. The `vocab_size` parameter specifies the size of the vocabulary to be learned during BPE training, which was previously set to `vocabulary_size`. The `special_tokens` parameter specifies a list of special tokens to be included in the vocabulary, such as `[UNK]`, `[PAD]`, `[CLS]`, `[SEP]`, and `[MASK]`.
#
# This configuration sets up the BPE trainer to learn a vocabulary of the specified size, including the special tokens defined, which will be used during tokenization and text processing tasks.

trainer = trainers.BpeTrainer(vocab_size=vocabulary_size, special_tokens=special_tokens)

# ## Dataset Creation and Training
#
# In this section of the code, a dataset is created from the text data and used to train the tokenizer. Here's an explanation of the provided code:
#
# - `dataset = Dataset.from_pandas(test[['text']])`: This line of code creates a dataset from the 'text' column of the `test` DataFrame using the `Dataset.from_pandas()` method. The dataset contains only the text data that will be used for training the tokenizer.
#
# - `def train_corp_iter():`: This line of code defines a generator function named `train_corp_iter()`. This function iterates over the dataset in batches of 50 samples at a time, yielding the text data for each batch.
#
# - `for i in range(0, len(dataset), 50):`: This line of code sets up a loop to iterate over the dataset in batches of 50 samples. The loop iterates from 0 to the length of the dataset, with a step size of 50.
#
# - `yield dataset[i : i + 50]["text"]`: This line of code yields the text data for each batch of 50 samples from the dataset. It selects the 'text' column for each batch using slicing notation.
#
# - `raw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)`: Finally, this line of code trains the tokenizer (`raw_tokenizer`) from the text data iterator generated by the `train_corp_iter()` function. The `trainer` parameter specifies the BPE trainer (`trainer`) to be used for training.
#
# This process iterates over the dataset in batches, feeding the text data to the BPE trainer for vocabulary learning, thereby training the tokenizer based on the provided text data.

dataset = Dataset.from_pandas(test[['text']])
def train_corp_iter():
    for i in range(0, len(dataset), 50):
        yield dataset[i : i + 50]["text"]
raw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)

# ## Pretrained Tokenizer Configuration
#
# In this section of the code, a pretrained tokenizer is configured based on the trained tokenizer object (`raw_tokenizer`). Here's an explanation of the provided code:
#
# - `tokenizer = PreTrainedTokenizerFast(tokenizer_object=raw_tokenizer, unk_token="[UNK]", pad_token="[PAD]", cls_token="[CLS]", sep_token="[SEP]", mask_token="[MASK]")`: This line of code initializes a pretrained tokenizer using the `PreTrainedTokenizerFast` class. The `tokenizer_object` parameter specifies the trained tokenizer object (`raw_tokenizer`) that was previously configured. The `unk_token`, `pad_token`, `cls_token`, `sep_token`, and `mask_token` parameters specify the special tokens to be used during tokenization.
#
# This configuration sets up the pretrained tokenizer with the same special tokens and tokenization rules as the trained tokenizer, ensuring consistency in tokenization and text processing.

tokenizer = PreTrainedTokenizerFast(tokenizer_object=raw_tokenizer, unk_token="[UNK]", pad_token="[PAD]", cls_token="[CLS]", sep_token="[SEP]", mask_token="[MASK]")

# ## Tokenization
#
# In this section of the code, text data from the test and train datasets is tokenized using the pretrained tokenizer. Here's an explanation of the provided code:
#
# ### Tokenization for Test Data:
#
# - `tokenized_texts_test = []`: This line of code initializes an empty list `tokenized_texts_test` to store the tokenized texts for the test dataset.
#
# - `for text in tqdm(test['text'].tolist()):`: This loop iterates over each text sample in the 'text' column of the test dataset using tqdm for progress visualization.
#
# - `tokenized_texts_test.append(tokenizer.tokenize(text))`: Inside the loop, each text sample is tokenized using the pretrained tokenizer (`tokenizer.tokenize(text)`), and the resulting tokenized text is appended to the `tokenized_texts_test` list.
#
# ### Tokenization for Train Data:
#
# - `tokenized_texts_train = []`: This line of code initializes an empty list `tokenized_texts_train` to store the tokenized texts for the train dataset.
#
# - `for text in tqdm(train['text'].tolist()):`: This loop iterates over each text sample in the 'text' column of the train dataset using tqdm for progress visualization.
#
# - `tokenized_texts_train.append(tokenizer.tokenize(text))`: Inside the loop, each text sample is tokenized using the pretrained tokenizer (`tokenizer.tokenize(text)`), and the resulting tokenized text is appended to the `tokenized_texts_train` list.
#
# These loops tokenize each text sample in the test and train datasets using the pretrained tokenizer, producing lists of tokenized texts for further processing and analysis.

# +
tokenized_texts_test = []
for text in tqdm(test['text'].tolist()):
    tokenized_texts_test.append(tokenizer.tokenize(text))

tokenized_texts_train = []
for text in tqdm(train['text'].tolist()):
    tokenized_texts_train.append(tokenizer.tokenize(text))
# -

# Since `tokenized_texts_test` is a list of tokenized texts, you can access a specific tokenized text by its index. For example, to access the tokenized text at index 1, you can simply use `tokenized_texts_test[1]`. 
#
# If you would like to see the tokenized text at index 1, here's how you can do it:

tokenized_texts_test[1]


# Explanation
#
# ## TF-IDF Vectorization
#
# In this section of the code, TF-IDF vectorization is applied to tokenize text data. Here's an explanation of the provided code:
#
# - `def dummy(text):`: This line of code defines a dummy function named `dummy`. This function simply returns the input text as is. This dummy function is used as a placeholder for the tokenizer and preprocessor parameters in the `TfidfVectorizer` to bypass tokenization and preprocessing.
#
# - `vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=LOWERCASE, sublinear_tf=True, analyzer='word', tokenizer=dummy, preprocessor=dummy, token_pattern=None, strip_accents='unicode')`: This line of code initializes a TF-IDF vectorizer (`vectorizer`) with specific configuration parameters. The `ngram_range=(3, 5)` parameter specifies that n-grams of sizes ranging from 3 to 5 will be considered during vectorization. The `lowercase=LOWERCASE` parameter specifies whether the text should be converted to lowercase (based on the `LOWERCASE` variable). The `sublinear_tf=True` parameter applies sublinear TF scaling, which helps mitigate the impact of term frequency on the TF-IDF values. The `analyzer='word'` parameter indicates that the vectorizer should treat each word as a separate feature. The `tokenizer=dummy` and `preprocessor=dummy` parameters specify the dummy function as the tokenizer and preprocessor to bypass tokenization and preprocessing. The `token_pattern=None` parameter disables token pattern matching, and `strip_accents='unicode'` parameter enables Unicode normalization for stripping accents.
#
# - `vectorizer.fit(tokenized_texts_test)`: This line of code fits the TF-IDF vectorizer to the tokenized text data from the test dataset (`tokenized_texts_test`). This step learns the vocabulary and IDF (Inverse Document Frequency) values from the test data, which will be used to transform both the train and test data into TF-IDF matrices.
#
# - `vocab = vectorizer.vocabulary_`: This line of code retrieves the vocabulary learned by the TF-IDF vectorizer from the test data.
#
# - `vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=LOWERCASE, sublinear_tf=True, vocabulary=vocab, analyzer='word', tokenizer=dummy, preprocessor=dummy, token_pattern=None, strip_accents='unicode')`: This line of code initializes a new TF-IDF vectorizer (`vectorizer`) using the vocabulary learned from the test data. This ensures that the same vocabulary is used for both the train and test data during vectorization.
#
# - `tf_train = vectorizer.fit_transform(tokenized_texts_train)`: This line of code transforms the tokenized text data from the train dataset (`tokenized_texts_train`) into a TF-IDF matrix (`tf_train`) using the TF-IDF vectorizer. The `fit_transform()` method learns the IDF values from the train data and transforms the train data into TF-IDF matrices.
#
# - `tf_test = vectorizer.transform(tokenized_texts_test)`: Similarly, this line of code transforms the tokenized text data from the test dataset (`tokenized_texts_test`) into a TF-IDF matrix (`tf_test`) using the same TF-IDF vectorizer. The `transform()` method applies the learned IDF values from the train data to transform the test data into TF-IDF matrices.
#
# These steps perform TF-IDF vectorization on the tokenized text data, enabling the representation of text data as numerical feature vectors suitable for machine learning models.

# +
def dummy(text):
    return text

vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=LOWERCASE, sublinear_tf=True, 
    analyzer = 'word',
    tokenizer = dummy,
    preprocessor = dummy,
    token_pattern = None, strip_accents='unicode')

vectorizer.fit(tokenized_texts_test)

# Getting vocab
vocab = vectorizer.vocabulary_

print(vocab)

vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=LOWERCASE, sublinear_tf=True, vocabulary=vocab,
                            analyzer = 'word',
                            tokenizer = dummy,
                            preprocessor = dummy,
                            token_pattern = None, strip_accents='unicode'
                            )

tf_train = vectorizer.fit_transform(tokenized_texts_train)
tf_test = vectorizer.transform(tokenized_texts_test)
# -

# ## Getting the Labels
#
# The variable `y_train` is assigned the values of the 'label' column from the `train` DataFrame. Here's an explanation:
#
# - `train['label']`: This retrieves the 'label' column from the `train` DataFrame, which presumably contains the labels or target values associated with the training data.
#
# - `.values`: This converts the values of the 'label' column into a NumPy array. This is done to ensure that `y_train` contains a standard NumPy array of labels, which is commonly used in machine learning tasks.
#
# After executing this line of code, `y_train` will contain the labels associated with the training data, ready to be used for training machine learning models.

y_train = train['label'].values

# ## Training
#
# - `clf = MultinomialNB()`: This line initializes a Multinomial Naive Bayes classifier (`clf`). Multinomial Naive Bayes is a popular algorithm for text classification tasks.
#
# - `sgd_model = SGDClassifier(loss="modified_huber")`: This line initializes a Stochastic Gradient Descent classifier (`sgd_model`) with the "modified_huber" loss function. Stochastic Gradient Descent is a versatile optimization algorithm widely used in machine learning.
#
# - `n_models = 2`: This variable defines the number of models in the ensemble. In this case, it's set to 2.
#
# - `weights = np.ones(n_models) / n_models`: This line creates an array of weights, with each weight initialized to 1 divided by the number of models. These weights will be used in the ensemble to combine the predictions from different models.
#
# - `ensemble = VotingClassifier(estimators=[('mnb', clf), ('sgd', sgd_model)], weights=weights, voting='soft', n_jobs=-1)`: This line initializes a Voting Classifier (`ensemble`) with soft voting. The estimators parameter specifies the list of models to be included in the ensemble, along with their corresponding names. The weights parameter specifies the weights assigned to each model in the ensemble. Soft voting takes into account the probability estimates from each model. The n_jobs parameter indicates the number of jobs to run in parallel during training, with -1 indicating to use all available CPU cores.
#
# - `ensemble.fit(tf_train, y_train)`: This line fits the ensemble model to the training data (`tf_train`) and the corresponding labels (`y_train`), training the individual models in the ensemble and combining their predictions during training.
#
# Overall, this code sets up an ensemble of two classifiers (Multinomial Naive Bayes and Stochastic Gradient Descent) using a soft voting strategy, where each model contributes to the final prediction based on its assigned weight. The ensemble is trained on the TF-IDF transformed training data.

# +
clf = MultinomialNB()

sgd_model = SGDClassifier(
    loss="modified_huber") 

# Number of models
n_models = 2

# Create an array of weights, each set to 1/n_models
weights = np.ones(n_models) / n_models
weights = weights

# Create the ensemble model
ensemble = VotingClassifier(estimators=[('mnb',clf), ('sgd', sgd_model)],weights=weights, voting='soft', n_jobs=-1)
ensemble.fit(tf_train, y_train)
# -

# ## Getting the Predictions
#
# - `final_preds = ensemble.predict_proba(tf_test)[:, 1]`: This line predicts the probabilities of the test samples belonging to each class using the trained ensemble model (`ensemble`). The `predict_proba()` method returns the probability estimates for each class for each test sample. Since you're interested in the probability of the positive class, which is usually represented by the class with index 1, `[:, 1]` is used to select the probabilities of the positive class only. These probabilities are stored in the variable `final_preds`.
#
# - `print(ensemble.predict_proba(tf_test))`: This line prints the probability estimates for each class for each test sample generated by the ensemble model.
#
# - `print(final_preds)`: This line prints the probabilities of the positive class for each test sample, which were stored in the variable `final_preds` in the previous line.
#
# Overall, this code snippet computes the probability estimates for the positive class for each test sample using the trained ensemble model and prints these probabilities.
#
# Please note that the output of `ensemble.predict_proba(tf_test)` will be an array where each row represents a test sample, and each column represents the probability of belonging to a particular class. The `final_preds` array will contain only the probabilities of the positive class for each test sample.

final_preds = ensemble.predict_proba(tf_test)[:,1]
print(ensemble.predict_proba(tf_test))
print(final_preds)

# ## Getting the Results
#
# - `rounded_preds = [1 if x >= 0.5 else 0 for x in final_preds]`: This line creates a new list `rounded_preds` where each probability in `final_preds` is rounded to 1 if it's greater than or equal to 0.5, otherwise, it's rounded to 0. This is a common threshold for binary classification tasks.
#
# - `print(rounded_preds)`: This line prints the rounded predictions.
#
# - `submission = pd.DataFrame({'labels': rounded_preds, 'real_labels': labels})`: This line creates a new DataFrame called `submission` containing the rounded predictions (`labels`) and the real labels (`labels`) extracted from the test dataset.
#
# - `submission.to_csv('submission.csv', index=False)`: This line saves the `submission` DataFrame to a CSV file named 'submission.csv', without including the DataFrame index.
#
# - `print(confusion_matrix(labels, rounded_preds))`: This line prints the confusion matrix, which is a table showing the counts of true positive, true negative, false positive, and false negative predictions.
#
# - `print(classification_report(labels, rounded_preds))`: This line prints the classification report, which includes precision, recall, F1-score, and support for each class, as well as the average values.
#
# - `print(accuracy_score(labels, rounded_preds))`: This line prints the accuracy score, which represents the proportion of correctly classified samples.
#
# These lines of code perform evaluation of the predictions by comparing them with the real labels, and then save the predictions to a CSV file named 'submission.csv'.

# +
rounded_preds = [1 if x >= 0.5 else 0 for x in final_preds]
print(rounded_preds)

# Create a new dataframe with the predictions and call it submissions and save it to submissions.csv
submission = pd.DataFrame({'labels': rounded_preds, 'real_labels': labels})

submission.to_csv('submission.csv', index=False)

# Compare the predictions to the real labels
print(confusion_matrix(labels, rounded_preds))
print(classification_report(labels, rounded_preds))
print(accuracy_score(labels, rounded_preds))
